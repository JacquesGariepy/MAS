# ü¶ô Ollama Quick Start avec Docker

## üöÄ D√©marrage Rapide (2 options)

### Option 1 : Ollama dans Docker (Recommand√©)

```bash
# 1. Lancer tout le stack avec Ollama inclus
docker-compose -f docker-compose.ollama.yml up -d

# 2. Le mod√®le llama2 sera t√©l√©charg√© automatiquement
# V√©rifier les logs
docker-compose -f docker-compose.ollama.yml logs -f ollama-setup

# 3. C'est tout ! L'API est sur http://localhost:8000
```

### Option 2 : Ollama sur votre machine + Docker pour le reste

```bash
# 1. Installer Ollama sur votre syst√®me
curl -fsSL https://ollama.ai/install.sh | sh

# 2. T√©l√©charger un mod√®le
ollama pull llama2
# ou d'autres mod√®les : mistral, mixtral, codellama, etc.

# 3. Lancer Ollama (il d√©marre automatiquement apr√®s l'installation)
# V√©rifier qu'il tourne
curl http://localhost:11434/api/tags

# 4. Configurer .env
echo "LLM_PROVIDER=ollama" > .env
echo "LLM_BASE_URL=http://host.docker.internal:11434" >> .env
echo "LLM_MODEL=llama2" >> .env

# 5. Lancer les services (sans Ollama dans Docker)
docker-compose -f docker-compose.dev.yml up -d
```

## üéÆ Avec GPU NVIDIA

### Pour Ollama dans Docker avec GPU

1. **Installer NVIDIA Container Toolkit**
```bash
# Ubuntu/Debian
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

2. **Modifier docker-compose.ollama.yml**
```yaml
# D√©commenter ces lignes dans le service ollama
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

3. **Lancer avec GPU**
```bash
docker-compose -f docker-compose.ollama.yml up -d
```

## üìä Mod√®les Populaires

| Mod√®le | Taille | Usage | Commande |
|--------|---------|--------|----------|
| `llama2` | 3.8GB | G√©n√©ral | `ollama pull llama2` |
| `mistral` | 4.1GB | Rapide & bon | `ollama pull mistral` |
| `mixtral` | 26GB | Tr√®s puissant | `ollama pull mixtral` |
| `codellama` | 3.8GB | Code | `ollama pull codellama` |
| `phi` | 1.6GB | Petit & rapide | `ollama pull phi` |

### Changer de mod√®le

```bash
# Dans .env
OLLAMA_MODEL=mistral

# Ou directement
OLLAMA_MODEL=mistral docker-compose -f docker-compose.ollama.yml up -d
```

## üîß Commandes Utiles

### Gestion des mod√®les
```bash
# Lister les mod√®les install√©s
docker exec ollama ollama list

# T√©l√©charger un nouveau mod√®le
docker exec ollama ollama pull mistral

# Supprimer un mod√®le
docker exec ollama ollama rm llama2

# Tester un mod√®le directement
docker exec -it ollama ollama run llama2 "Hello, how are you?"
```

### Monitoring
```bash
# Voir l'utilisation GPU (si NVIDIA)
nvidia-smi

# Logs Ollama
docker-compose -f docker-compose.ollama.yml logs -f ollama

# V√©rifier que Ollama r√©pond
curl http://localhost:11434/api/tags

# Tester via l'API MAS
curl http://localhost:8000/health
```

## üéØ Configuration dans MAS

Le syst√®me est pr√©-configur√© pour Ollama. L'agent va utiliser :
- Provider : `ollama`
- URL : `http://ollama:11434` (communication inter-containers)
- Mod√®le : `llama2` (ou celui configur√© dans OLLAMA_MODEL)

### Exemple d'appel API
```bash
curl -X POST http://localhost:8000/api/v1/agents \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Assistant",
    "role": "helper",
    "agent_type": "cognitive",
    "capabilities": ["analyze", "generate"]
  }'
```

## üìà Performance

### Recommandations
- **CPU only** : Mod√®les < 7B param√®tres (phi, llama2-7b)
- **GPU 8GB** : Mod√®les jusqu'√† 13B (llama2-13b, mistral)
- **GPU 16GB+** : Grands mod√®les (mixtral, llama2-70b quantiz√©)

### Optimisation m√©moire
```bash
# Limiter la m√©moire Docker
docker update --memory="8g" ollama
```

## üîç Troubleshooting

### "Ollama is not running"
```bash
# V√©rifier le statut
docker ps | grep ollama

# Red√©marrer
docker-compose -f docker-compose.ollama.yml restart ollama
```

### "Model not found"
```bash
# T√©l√©charger le mod√®le
docker exec ollama ollama pull llama2
```

### Lenteur
- V√©rifier la RAM disponible : `docker stats`
- Utiliser un mod√®le plus petit : `phi` ou `tinyllama`
- Activer le GPU si disponible

## üåê Acc√®s depuis l'ext√©rieur

Pour acc√©der √† Ollama depuis d'autres machines :
```yaml
# Dans docker-compose.ollama.yml
ollama:
  ports:
    - "0.0.0.0:11434:11434"  # √âcoute sur toutes les interfaces
```

‚ö†Ô∏è **S√©curit√©** : En production, utilisez un reverse proxy avec authentification.

---

**Rappel** : Avec cette configuration, tout est dans Docker. Pas besoin d'installer quoi que ce soit sur votre syst√®me ! üê≥ü¶ô