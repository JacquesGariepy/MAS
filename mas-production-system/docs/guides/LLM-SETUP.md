# ü§ñ Configuration LLM : LM Studio vs Ollama vs OpenAI

## üìã Vue d'ensemble

| Provider | O√π √ßa tourne ? | Besoin Docker ? | Gratuit ? | Config |
|----------|---------------|-----------------|-----------|---------|
| **OpenAI** | ‚òÅÔ∏è Cloud | ‚ùå Non | üí∞ Payant | Simple |
| **LM Studio** | üíª Sur votre PC | ‚ùå Non | ‚úÖ Gratuit | Moyen |
| **Ollama** | üíª Sur votre PC | üîÑ Optionnel | ‚úÖ Gratuit | Simple |

## üéØ Configuration pour chaque provider

### 1Ô∏è‚É£ OpenAI (Plus simple)

```bash
# .env
LLM_PROVIDER=openai
LLM_API_KEY=sk-xxxxxxxxxxxxxxxx
LLM_MODEL=gpt-4o-mini

# C'est tout ! OpenAI est dans le cloud
```

### 2Ô∏è‚É£ LM Studio (Sur votre machine)

**√âtape 1 : Installer LM Studio sur votre PC (pas dans Docker)**
- T√©l√©charger : https://lmstudio.ai/
- Lancer LM Studio
- T√©l√©charger un mod√®le (ex: Mistral, Llama 2)
- D√©marrer le serveur local (port 1234 par d√©faut)

**√âtape 2 : Configurer .env**
```bash
# .env
LLM_PROVIDER=lmstudio
LLM_BASE_URL=http://host.docker.internal:1234/v1
LLM_API_KEY=not-needed
LLM_MODEL=local-model
```

**Important** : `host.docker.internal` permet √† Docker d'acc√©der √† votre PC

### 3Ô∏è‚É£ Ollama (Option A : Sur votre machine)

**√âtape 1 : Installer Ollama sur votre PC**
```bash
# Linux/Mac
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# T√©l√©charger depuis https://ollama.ai/download
```

**√âtape 2 : T√©l√©charger un mod√®le**
```bash
ollama pull llama2
# ou
ollama pull mistral
```

**√âtape 3 : Configurer .env**
```bash
# .env
LLM_PROVIDER=ollama
LLM_BASE_URL=http://host.docker.internal:11434
LLM_API_KEY=not-needed
LLM_MODEL=llama2
```

### 3Ô∏è‚É£ Ollama (Option B : Dans Docker) üê≥

**Ajouter Ollama au docker-compose.dev.yml :**

```yaml
version: '3.8'
services:
  # ... vos autres services ...

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]  # Si vous avez un GPU NVIDIA
    # Pour CPU seulement, retirer la section deploy

  core:
    # ... votre service core ...
    environment:
      - LLM_PROVIDER=ollama
      - LLM_BASE_URL=http://ollama:11434  # Nom du service Docker
      - LLM_MODEL=llama2
    depends_on:
      - ollama
      - db
      - redis

volumes:
  ollama-data:  # Pour persister les mod√®les t√©l√©charg√©s
```

**T√©l√©charger un mod√®le dans Ollama Docker :**
```bash
# Apr√®s docker-compose up
docker-compose exec ollama ollama pull llama2
```

## üîß Docker Compose Complet avec Options

```yaml
# docker-compose.llm.yml - Exemple avec toutes les options

version: '3.8'
services:
  core:
    build:
      context: ./services/core
      dockerfile: Dockerfile.dev
    ports:
      - "8000:8000"
    environment:
      # Option 1 : OpenAI (dans le cloud)
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_MODEL=${LLM_MODEL:-gpt-4o-mini}
      
      # Option 2 : LM Studio (sur votre PC)
      # - LLM_PROVIDER=lmstudio
      # - LLM_BASE_URL=http://host.docker.internal:1234/v1
      # - LLM_MODEL=local-model
      
      # Option 3 : Ollama (dans Docker)
      # - LLM_PROVIDER=ollama
      # - LLM_BASE_URL=http://ollama:11434
      # - LLM_MODEL=llama2
      
      # Reste de la config
      - DATABASE_URL=postgresql://user:pass@db:5432/mas
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - db
      - redis
      # - ollama  # D√©commenter si vous utilisez Ollama dans Docker

  # Service Ollama optionnel (d√©commenter pour l'utiliser)
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   environment:
  #     - OLLAMA_MODELS=/root/.ollama/models

  db:
    image: postgres:15-alpine
    # ... reste de la config ...

  redis:
    image: redis:7-alpine
    # ... reste de la config ...

volumes:
  postgres-data:
  redis-data:
  ollama-data:  # Pour Ollama si utilis√©
```

## üìä Comparaison des approches

### LM Studio / Ollama sur votre PC (hors Docker)
**‚úÖ Avantages :**
- Utilise directement votre GPU
- Pas de duplication de mod√®les
- Plus simple √† g√©rer
- Interface graphique (LM Studio)

**‚ùå Inconv√©nients :**
- Doit √™tre lanc√© manuellement
- Configuration diff√©rente selon l'OS

### Ollama dans Docker
**‚úÖ Avantages :**
- Tout est dans Docker
- D√©marrage automatique
- Reproductible

**‚ùå Inconv√©nients :**
- Plus complexe avec GPU
- Mod√®les dupliqu√©s
- Plus de RAM n√©cessaire

## üöÄ Recommandations

1. **Pour d√©buter** ‚Üí OpenAI (simple mais payant)
2. **Pour du gratuit simple** ‚Üí Ollama sur votre PC
3. **Pour plus de contr√¥le** ‚Üí LM Studio sur votre PC
4. **Pour tout dans Docker** ‚Üí Ollama dans Docker

## üîç V√©rifier la connexion

```bash
# Tester la connexion au LLM
curl http://localhost:8000/health

# Voir les logs
docker-compose logs -f core | grep LLM

# Tester directement le LLM
# OpenAI
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $LLM_API_KEY"

# LM Studio (doit √™tre lanc√©)
curl http://localhost:1234/v1/models

# Ollama
curl http://localhost:11434/api/tags
```

## üìù Exemples de .env

### Pour OpenAI
```bash
LLM_PROVIDER=openai
LLM_API_KEY=sk-xxxxxxxxxxxxxxxx
LLM_MODEL=gpt-4o-mini
```

### Pour LM Studio
```bash
LLM_PROVIDER=lmstudio
LLM_BASE_URL=http://host.docker.internal:1234/v1
LLM_API_KEY=not-needed
LLM_MODEL=TheBloke/Mistral-7B-Instruct-v0.2-GGUF
```

### Pour Ollama (local)
```bash
LLM_PROVIDER=ollama
LLM_BASE_URL=http://host.docker.internal:11434
LLM_API_KEY=not-needed
LLM_MODEL=mistral
```

### Pour Ollama (Docker)
```bash
LLM_PROVIDER=ollama
LLM_BASE_URL=http://ollama:11434
LLM_API_KEY=not-needed
LLM_MODEL=llama2
```

---

**Note** : `host.docker.internal` fonctionne sur Docker Desktop (Windows/Mac). Sur Linux, utilisez l'IP de votre machine ou configurez le r√©seau Docker.