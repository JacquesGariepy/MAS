#!/usr/bin/env python3
"""
Script pour v√©rifier l'utilisation du LLM par les agents cognitifs.
Envoie des requ√™tes complexes n√©cessitant une r√©ponse g√©n√©r√©e par le LLM.
"""

import asyncio
import aiohttp
import json
import time
from datetime import datetime

# Configuration
API_BASE_URL = "http://localhost:8000/api/v1"


async def check_llm_integration():
    """V√©rifier que le LLM est correctement int√©gr√©"""
    print("="*60)
    print("üß† V√âRIFICATION DE L'INT√âGRATION LLM")
    print("="*60)
    
    async with aiohttp.ClientSession() as session:
        # 1. Cr√©er un utilisateur de test
        print("\nüìã Cr√©ation d'un utilisateur de test...")
        
        timestamp = int(time.time())
        username = f"llm_test_{timestamp}"
        email = f"{username}@mas.ai"
        password = "password123"
        
        # Register
        register_data = {
            "username": username,
            "email": email,
            "password": password
        }
        
        register_resp = await session.post(
            f"{API_BASE_URL.replace('/api/v1', '')}/auth/register",
            json=register_data
        )
        
        if register_resp.status not in [201, 400]:
            print(f"‚ùå Erreur cr√©ation utilisateur: {await register_resp.text()}")
            return
        
        # Login
        login_data = {"username": username, "password": password}
        login_resp = await session.post(
            f"{API_BASE_URL.replace('/api/v1', '')}/auth/token",
            data=login_data
        )
        
        if login_resp.status != 200:
            print(f"‚ùå Erreur connexion: {login_resp.status}")
            return
        
        token = (await login_resp.json())["access_token"]
        headers = {"Authorization": f"Bearer {token}"}
        print(f"‚úÖ Utilisateur {username} cr√©√© et connect√©")
        
        # 2. Cr√©er un agent cognitif
        print("\nüìã Cr√©ation d'un agent cognitif...")
        
        agent_config = {
            "name": "LLM Test Agent",
            "agent_type": "cognitive",
            "role": "Agent de test pour v√©rifier l'utilisation du LLM",
            "capabilities": ["analyse", "g√©n√©ration", "raisonnement"],
            "initial_beliefs": {
                "purpose": "test_llm",
                "llm_model": "qwen3-8b"
            },
            "initial_desires": ["r√©pondre", "analyser", "g√©n√©rer"],
            "configuration": {
                "llm_enabled": True,
                "max_tokens": 500,
                "temperature": 0.7
            }
        }
        
        create_resp = await session.post(
            f"{API_BASE_URL}/agents",
            json=agent_config,
            headers=headers
        )
        
        if create_resp.status != 201:
            print(f"‚ùå Erreur cr√©ation agent: {await create_resp.text()}")
            return
        
        agent = await create_resp.json()
        agent_id = agent["id"]
        print(f"‚úÖ Agent cognitif cr√©√©: {agent['name']} (ID: {agent_id[:8]}...)")
        
        # 3. D√©marrer l'agent
        print("\nüìã D√©marrage de l'agent...")
        
        start_resp = await session.post(
            f"{API_BASE_URL}/agents/{agent_id}/start",
            headers=headers
        )
        
        if start_resp.status == 200:
            print("‚úÖ Agent d√©marr√© avec succ√®s")
        elif start_resp.status == 400:
            error = await start_resp.json()
            if "already" in error.get("detail", "").lower():
                print("‚úÖ Agent d√©j√† actif")
            else:
                print(f"‚ùå Erreur d√©marrage: {error.get('detail')}")
                return
        else:
            print(f"‚ùå Erreur HTTP {start_resp.status}")
            return
        
        # Attendre l'initialisation
        await asyncio.sleep(2)
        
        # 4. Envoyer des requ√™tes n√©cessitant le LLM
        print("\nüìã Test de requ√™tes LLM...")
        
        test_queries = [
            {
                "performative": "query-ref",
                "content": {
                    "query": "Explique les avantages et inconv√©nients de l'architecture microservices",
                    "context": "Nous d√©veloppons une nouvelle application web"
                }
            },
            {
                "performative": "request",
                "content": {
                    "action": "analyze",
                    "data": "Les ventes ont augment√© de 15% ce trimestre mais les co√ªts ont augment√© de 20%",
                    "request": "Quelle est ton analyse de cette situation?"
                }
            },
            {
                "performative": "query-if",
                "content": {
                    "proposition": "Python est-il le meilleur langage pour le machine learning?",
                    "context": "Comparaison avec R, Julia et Scala"
                }
            }
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nüîç Test {i}: {query['content'].get('query', query['content'].get('action', 'requ√™te'))[:50]}...")
            
            # L'agent s'envoie le message √† lui-m√™me pour simplifier le test
            message_data = {
                "receiver_id": agent_id,
                "performative": query["performative"],
                "content": query["content"]
            }
            
            send_resp = await session.post(
                f"{API_BASE_URL}/agents/{agent_id}/messages",
                json=message_data,
                headers=headers
            )
            
            if send_resp.status == 201:
                print("   ‚úÖ Requ√™te envoy√©e")
            else:
                print(f"   ‚ùå √âchec envoi: {await send_resp.text()}")
        
        # 5. Attendre le traitement
        print("\n‚è≥ Attente du traitement par le LLM (10 secondes)...")
        await asyncio.sleep(10)
        
        # 6. R√©cup√©rer et analyser les messages
        print("\nüìã Analyse des r√©ponses...")
        
        messages_resp = await session.get(
            f"{API_BASE_URL}/agents/{agent_id}/messages",
            headers=headers
        )
        
        if messages_resp.status != 200:
            print(f"‚ùå Erreur r√©cup√©ration messages: {messages_resp.status}")
            return
        
        messages = (await messages_resp.json()).get("items", [])
        
        # Analyser les messages
        received_messages = [m for m in messages if m["receiver_id"] == agent_id]
        sent_messages = [m for m in messages if m["sender_id"] == agent_id and m["receiver_id"] != agent_id]
        
        print(f"\nüìä Statistiques:")
        print(f"   üì• Messages re√ßus: {len(received_messages)}")
        print(f"   üì§ R√©ponses envoy√©es: {len(sent_messages)}")
        
        # Chercher des r√©ponses g√©n√©r√©es par le LLM
        llm_responses = []
        
        for msg in messages:
            content = msg.get("content", {})
            if isinstance(content, dict):
                # Chercher des signes de g√©n√©ration LLM
                content_str = json.dumps(content)
                if any(indicator in content_str.lower() for indicator in [
                    "avantage", "inconv√©nient", "analyse", "microservice",
                    "python", "machine learning", "augment", "co√ªt"
                ]):
                    if len(content_str) > 100:  # R√©ponse substantielle
                        llm_responses.append(msg)
        
        print(f"   üß† R√©ponses LLM d√©tect√©es: {len(llm_responses)}")
        
        # Afficher les r√©ponses LLM
        if llm_responses:
            print("\n‚úÖ SUCC√àS: Le LLM est utilis√©!")
            print("\nüìù Exemples de r√©ponses g√©n√©r√©es par le LLM:")
            
            for i, response in enumerate(llm_responses[:2], 1):
                print(f"\n   R√©ponse {i}:")
                print(f"   Type: {response['performative']}")
                content = response['content']
                if isinstance(content, dict):
                    for key, value in content.items():
                        print(f"   {key}: {str(value)[:200]}...")
                else:
                    print(f"   Contenu: {str(content)[:200]}...")
        else:
            print("\n‚ö†Ô∏è  ATTENTION: Aucune r√©ponse LLM d√©tect√©e!")
            print("\nüîç Diagnostic:")
            print("   1. V√©rifiez que LMStudio est d√©marr√©")
            print("   2. V√©rifiez que le mod√®le qwen3-8b est charg√©")
            print("   3. V√©rifiez l'URL: http://host.docker.internal:1234/v1")
            print("   4. Consultez les logs Docker: docker-compose logs core | grep -i llm")
        
        # 7. V√©rifier la configuration
        print("\nüìã Configuration LLM attendue:")
        print("   Provider: lmstudio")
        print("   Model: qwen3-8b")
        print("   Base URL: http://host.docker.internal:1234/v1")
        print("   Dans Docker: host.docker.internal pointe vers l'h√¥te")
        
        # 8. Test de connectivit√© directe (optionnel)
        print("\nüìã Test de connectivit√© LMStudio...")
        try:
            # Tester depuis l'ext√©rieur du conteneur
            test_url = "http://localhost:1234/v1/models"
            async with session.get(test_url, timeout=5) as resp:
                if resp.status == 200:
                    models = await resp.json()
                    print(f"   ‚úÖ LMStudio accessible depuis l'h√¥te")
                    print(f"   üìå Mod√®les disponibles: {models}")
                else:
                    print(f"   ‚ùå LMStudio retourne le code {resp.status}")
        except Exception as e:
            print(f"   ‚ùå Impossible de contacter LMStudio: {type(e).__name__}")
            print("   ‚ÑπÔ∏è  C'est normal si le test s'ex√©cute dans Docker")


async def main():
    """Point d'entr√©e principal"""
    print("üß† Test d'int√©gration LLM dans MAS")
    print(f"   Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("")
    
    await check_llm_integration()
    
    print("\n" + "="*60)
    print("‚ú® Test termin√©")
    print("="*60)


if __name__ == "__main__":
    asyncio.run(main())