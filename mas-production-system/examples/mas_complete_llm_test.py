#!/usr/bin/env python3
"""
Test complet du syst√®me MAS avec communication LLM r√©elle
"""

import asyncio
import aiohttp
import json
import time
import os
from typing import Dict, List, Optional

API_BASE_URL = "http://localhost:8000"
API_V1 = f"{API_BASE_URL}/api/v1"


class MASCompleteLLMTest:
    def __init__(self):
        self.session = None
        self.users = {}
        self.agents = {}
        self.messages_sent = []
        self.timestamp = int(time.time() * 1000) % 1000000
        
    async def setup(self):
        """Configuration initiale"""
        self.session = aiohttp.ClientSession()
        print("="*80)
        print("ü§ñ TEST COMPLET MAS - COMMUNICATION LLM R√âELLE")
        print("="*80)
        print("\nüì¢ Ce test va d√©montrer:")
        print("   1. Cr√©ation de 3 agents intelligents")
        print("   2. Communication complexe n√©cessitant LLM")
        print("   3. Raisonnement et r√©ponses g√©n√©r√©es par LMStudio")
        print("\n‚ö†Ô∏è  SURVEILLEZ LMSTUDIO - Les requ√™tes vont appara√Ætre!")
        print("="*80)
        
    async def create_users(self):
        """Cr√©er les utilisateurs"""
        print("\nüìã Phase 1: Cr√©ation des utilisateurs...")
        
        users_data = {
            "alice": {"role": "Chef de Projet", "email": "alice@mas.ai"},
            "bob": {"role": "Architecte Logiciel", "email": "bob@mas.ai"},
            "charlie": {"role": "Expert IA", "email": "charlie@mas.ai"}
        }
        
        for name, info in users_data.items():
            user_data = {
                "username": f"{name}_{self.timestamp}",
                "email": f"{name}_{self.timestamp}@mas.ai",
                "password": "password123"
            }
            
            # Register
            async with self.session.post(f"{API_BASE_URL}/auth/register", json=user_data) as resp:
                if resp.status in [200, 201]:
                    print(f"‚úÖ {name.capitalize()} cr√©√© ({info['role']})")
                    
            # Login
            login_form = aiohttp.FormData()
            login_form.add_field('username', user_data["username"])
            login_form.add_field('password', user_data["password"])
            
            async with self.session.post(f"{API_BASE_URL}/auth/token", data=login_form) as resp:
                if resp.status == 200:
                    auth_resp = await resp.json()
                    self.users[name] = {
                        "username": user_data["username"],
                        "role": info["role"],
                        "token": auth_resp["access_token"],
                        "headers": {"Authorization": f"Bearer {auth_resp['access_token']}"}
                    }
                    
    async def create_intelligent_agents(self):
        """Cr√©er des agents avec capacit√©s cognitives avanc√©es"""
        print("\nüìã Phase 2: Cr√©ation des agents intelligents...")
        
        # Agent 1: Chef de Projet (Alice) - Cognitif
        chef_data = {
            "name": f"ChefProjet_{self.timestamp}",
            "agent_type": "reactive",  # Cognitif
            "role": "project_manager",
            "capabilities": ["planning", "coordination", "decision_making", "risk_assessment"],
            "description": "Chef de projet expert en m√©thodologies agiles et gestion d'√©quipe",
            "initial_beliefs": {
                "methodology": "Agile/Scrum",
                "team_size": 3,
                "project": "Syst√®me de recommandation IA",
                "deadline": "3 mois",
                "budget": "100k‚Ç¨"
            },
            "initial_desires": ["livrer_projet", "optimiser_ressources", "minimiser_risques"],
            "configuration": {
                "reasoning_depth": 5,
                "planning_horizon": 10
            }
        }
        
        async with self.session.post(
            f"{API_V1}/agents",
            json=chef_data,
            headers=self.users["alice"]["headers"]
        ) as resp:
            if resp.status in [200, 201]:
                agent_resp = await resp.json()
                self.agents["chef"] = {
                    "id": agent_resp["id"],
                    "name": chef_data["name"],
                    "owner": "alice",
                    "type": "cognitive"
                }
                print(f"‚úÖ Chef de Projet cr√©√© (Agent Cognitif)")
                
        # Agent 2: Architecte Logiciel (Bob) - Cognitif
        architecte_data = {
            "name": f"Architecte_{self.timestamp}",
            "agent_type": "reactive",  # Cognitif
            "role": "software_architect",
            "capabilities": ["system_design", "technology_selection", "performance_optimization"],
            "description": "Architecte senior sp√©cialis√© en syst√®mes distribu√©s et IA",
            "initial_beliefs": {
                "expertise": ["microservices", "event-driven", "machine learning"],
                "tech_stack": ["Python", "FastAPI", "PostgreSQL", "Redis", "Kubernetes"],
                "patterns": ["CQRS", "Event Sourcing", "Domain-Driven Design"]
            },
            "initial_desires": ["concevoir_architecture_scalable", "assurer_performance", "maintenir_qualit√©"]
        }
        
        async with self.session.post(
            f"{API_V1}/agents",
            json=architecte_data,
            headers=self.users["bob"]["headers"]
        ) as resp:
            if resp.status in [200, 201]:
                agent_resp = await resp.json()
                self.agents["architecte"] = {
                    "id": agent_resp["id"],
                    "name": architecte_data["name"],
                    "owner": "bob",
                    "type": "cognitive"
                }
                print(f"‚úÖ Architecte Logiciel cr√©√© (Agent Cognitif)")
                
        # Agent 3: Expert IA (Charlie) - Hybride
        expert_ia_data = {
            "name": f"ExpertIA_{self.timestamp}",
            "agent_type": "hybrid",
            "role": "ai_expert",
            "capabilities": ["ml_modeling", "algorithm_selection", "data_analysis", "model_optimization"],
            "description": "Expert en IA sp√©cialis√© en syst√®mes de recommandation et NLP",
            "reactive_rules": {
                "performance_issue": "Analyser imm√©diatement les m√©triques et proposer optimisations",
                "data_quality": "V√©rifier la qualit√© des donn√©es avant tout entra√Ænement",
                "model_selection": "Comparer au moins 3 approches diff√©rentes"
            },
            "cognitive_threshold": 0.6,
            "initial_beliefs": {
                "algorithms": ["collaborative_filtering", "content_based", "hybrid_approaches"],
                "frameworks": ["TensorFlow", "PyTorch", "Scikit-learn"],
                "metrics": ["precision", "recall", "F1-score", "RMSE"]
            }
        }
        
        async with self.session.post(
            f"{API_V1}/agents",
            json=expert_ia_data,
            headers=self.users["charlie"]["headers"]
        ) as resp:
            if resp.status in [200, 201]:
                agent_resp = await resp.json()
                self.agents["expert"] = {
                    "id": agent_resp["id"],
                    "name": expert_ia_data["name"],
                    "owner": "charlie",
                    "type": "hybrid"
                }
                print(f"‚úÖ Expert IA cr√©√© (Agent Hybride)")
                
    async def start_all_agents(self):
        """D√©marrer tous les agents"""
        print("\nüìã Phase 3: D√©marrage des agents...")
        
        for role, agent in self.agents.items():
            headers = self.users[agent["owner"]]["headers"]
            async with self.session.post(
                f"{API_V1}/agents/{agent['id']}/start",
                headers=headers
            ) as resp:
                if resp.status == 200:
                    print(f"‚úÖ {role.capitalize()} d√©marr√© et pr√™t")
                    
        # Attendre l'initialisation
        print("\n‚è≥ Initialisation des agents (5 secondes)...")
        await asyncio.sleep(5)
        
    async def complex_scenario_1(self):
        """Sc√©nario 1: Planification du projet"""
        print("\nüéØ Sc√©nario 1: Planification initiale du projet")
        print("-" * 60)
        
        # Chef demande √† l'Architecte une proposition d'architecture
        message = {
            "sender_id": self.agents["chef"]["id"],
            "receiver_id": self.agents["architecte"]["id"],
            "performative": "request",
            "content": {
                "action": "proposer_architecture",
                "context": {
                    "projet": "Syst√®me de recommandation pour e-commerce",
                    "contraintes": [
                        "100k utilisateurs actifs/jour",
                        "Temps de r√©ponse < 200ms",
                        "Scalabilit√© horizontale requise",
                        "Int√©gration avec syst√®me existant"
                    ],
                    "d√©lai": "3 mois",
                    "√©quipe": "3 d√©veloppeurs seniors"
                },
                "question": "Peux-tu proposer une architecture technique d√©taill√©e avec les composants principaux, les technologies recommand√©es et une estimation de l'effort de d√©veloppement?"
            }
        }
        
        print(f"üì§ Chef ‚Üí Architecte: Demande d'architecture")
        print(f"   Projet: {message['content']['context']['projet']}")
        print(f"   Contraintes: {len(message['content']['context']['contraintes'])} d√©finies")
        
        resp = await self._send_message(message, "alice")
        if resp:
            self.messages_sent.append(resp['id'])
            print(f"   ‚úÖ Message envoy√© (ID: {resp['id'][:8]}...)")
            print("   üß† LMSTUDIO devrait recevoir une requ√™te maintenant!")
            
    async def complex_scenario_2(self):
        """Sc√©nario 2: Consultation sur les algorithmes IA"""
        print("\nüéØ Sc√©nario 2: S√©lection des algorithmes de recommandation")
        print("-" * 60)
        
        # Architecte consulte l'Expert IA
        message = {
            "sender_id": self.agents["architecte"]["id"],
            "receiver_id": self.agents["expert"]["id"],
            "performative": "query",
            "content": {
                "question": "Pour un syst√®me de recommandation e-commerce avec 100k utilisateurs, quels algorithmes recommandes-tu? Compare les approches collaborative filtering, content-based et hybride en termes de performance, scalabilit√© et cold start problem.",
                "donn√©es_disponibles": [
                    "Historique d'achats (2 ans)",
                    "√âvaluations produits (1-5 √©toiles)",
                    "Donn√©es comportementales (clics, temps pass√©)",
                    "M√©tadonn√©es produits (cat√©gories, prix, descriptions)"
                ],
                "m√©triques_cibles": {
                    "precision": ">0.8",
                    "temps_calcul": "<200ms",
                    "couverture_catalogue": ">60%"
                }
            }
        }
        
        print(f"üì§ Architecte ‚Üí Expert IA: Consultation algorithmes")
        print(f"   Donn√©es: {len(message['content']['donn√©es_disponibles'])} sources")
        print(f"   Question technique complexe n√©cessitant analyse approfondie")
        
        resp = await self._send_message(message, "bob")
        if resp:
            self.messages_sent.append(resp['id'])
            print(f"   ‚úÖ Message envoy√© (ID: {resp['id'][:8]}...)")
            print("   üß† Nouvelle requ√™te LLM en cours!")
            
    async def complex_scenario_3(self):
        """Sc√©nario 3: Analyse de risques"""
        print("\nüéØ Sc√©nario 3: Analyse des risques du projet")
        print("-" * 60)
        
        # Expert IA informe le Chef des risques techniques
        message = {
            "sender_id": self.agents["expert"]["id"],
            "receiver_id": self.agents["chef"]["id"],
            "performative": "inform",
            "content": {
                "type": "analyse_risques",
                "risques_identifi√©s": [
                    {
                        "risque": "Cold start problem",
                        "impact": "Nouveaux utilisateurs sans recommandations pertinentes",
                        "probabilit√©": "haute",
                        "mitigation": "Impl√©menter syst√®me hybride avec r√®gles m√©tier"
                    },
                    {
                        "risque": "Scalabilit√© des calculs",
                        "impact": "D√©gradation performance avec croissance utilisateurs",
                        "probabilit√©": "moyenne",
                        "mitigation": "Architecture lambda avec pr√©-calculs batch"
                    },
                    {
                        "risque": "Biais dans les recommandations",
                        "impact": "R√©duction diversit√© et satisfaction utilisateur",
                        "probabilit√©": "moyenne",
                        "mitigation": "Algorithmes de diversification et tests A/B"
                    }
                ],
                "recommandation": "Pr√©voir 20% de temps suppl√©mentaire pour gestion des risques",
                "demande_d√©cision": "Valider l'approche hybride malgr√© complexit√© accrue?"
            }
        }
        
        print(f"üì§ Expert IA ‚Üí Chef: Rapport d'analyse des risques")
        print(f"   Risques identifi√©s: {len(message['content']['risques_identifi√©s'])}")
        print(f"   Demande de d√©cision strat√©gique")
        
        resp = await self._send_message(message, "charlie")
        if resp:
            self.messages_sent.append(resp['id'])
            print(f"   ‚úÖ Message envoy√© (ID: {resp['id'][:8]}...)")
            print("   üß† Le Chef doit analyser et prendre une d√©cision!")
            
    async def _send_message(self, message_data: dict, sender_user: str) -> Optional[dict]:
        """Envoyer un message et retourner la r√©ponse"""
        try:
            headers = self.users[sender_user]["headers"]
            sender_id = message_data["sender_id"]
            
            async with self.session.post(
                f"{API_V1}/agents/{sender_id}/messages",
                json=message_data,
                headers=headers
            ) as resp:
                if resp.status in [200, 201]:
                    return await resp.json()
                else:
                    print(f"   ‚ùå Erreur envoi: {resp.status}")
                    return None
        except Exception as e:
            print(f"   ‚ùå Erreur: {str(e)}")
            return None
            
    async def wait_and_check_responses(self):
        """Attendre et v√©rifier les r√©ponses LLM"""
        print("\nüìã Phase 4: Surveillance de l'activit√© des agents...")
        
        # Attendre que les agents traitent les messages
        max_wait = 60  # Maximum 60 secondes
        check_interval = 3  # V√©rifier toutes les 3 secondes
        activity_detected = False
        message_history = {}  # Track messages per agent
        llm_responses_count = 0
        
        for elapsed in range(0, max_wait, check_interval):
            print(f"\r‚è≥ Surveillance de l'activit√©... {elapsed}/{max_wait}s", end="", flush=True)
            
            # V√©rifier les messages de chaque agent
            new_activity = False
            for role, agent in self.agents.items():
                headers = self.users[agent["owner"]]["headers"]
                agent_id = agent['id']
                
                try:
                    async with self.session.get(
                        f"{API_V1}/agents/{agent_id}/messages",
                        headers=headers
                    ) as resp:
                        if resp.status == 200:
                            messages = await resp.json()
                            
                            # Count messages for this agent
                            current_count = len(messages) if isinstance(messages, list) else len(messages.get('items', []))
                            previous_count = message_history.get(agent_id, 0)
                            
                            if current_count > previous_count:
                                new_activity = True
                                activity_detected = True
                                message_history[agent_id] = current_count
                                
                                # Check for LLM-generated responses
                                message_list = messages if isinstance(messages, list) else messages.get('items', [])
                                for msg in message_list[previous_count:]:  # Only check new messages
                                    if isinstance(msg, dict):
                                        content = msg.get('content', {})
                                        if isinstance(content, dict):
                                            # Look for signs of LLM responses
                                            llm_indicators = ['response', 'answer', 'analysis', 'recommendation', 
                                                            'proposal', 'evaluation', 'suggestion', 'explanation']
                                            if any(indicator in content for indicator in llm_indicators):
                                                llm_responses_count += 1
                                                print(f"\n   ü§ñ R√©ponse LLM d√©tect√©e de {role}!")
                except Exception as e:
                    pass
            
            if new_activity:
                total_messages = sum(message_history.values())
                print(f"\r‚úÖ Activit√© en cours! Messages: {total_messages} | R√©ponses LLM: {llm_responses_count}     ")
                # Donner plus de temps pour traiter apr√®s activit√©
                await asyncio.sleep(check_interval * 2)
            else:
                await asyncio.sleep(check_interval)
                
        # R√©sum√© final
        if activity_detected:
            total_messages = sum(message_history.values())
            print(f"\n‚úÖ Traitement termin√©! Total messages: {total_messages} | R√©ponses LLM: {llm_responses_count}")
        else:
            print("\n‚ö†Ô∏è  Aucune activit√© d√©tect√©e. V√©rifiez que LMStudio est actif.")
        
        print("\nüìã Phase 5: V√©rification des r√©ponses g√©n√©r√©es par LLM...")
        print("="*60)
        
        responses_found = 0
        
        for role, agent in self.agents.items():
            headers = self.users[agent["owner"]]["headers"]
            
            print(f"\nüìä Messages de {role.upper()}:")
            
            async with self.session.get(
                f"{API_V1}/agents/{agent['id']}/messages",
                headers=headers
            ) as resp:
                if resp.status == 200:
                    messages = await resp.json()
                    
                    # S√©parer messages re√ßus et envoy√©s
                    received = []
                    sent = []
                    
                    for msg in messages:
                        if isinstance(msg, dict):
                            if msg.get('receiver_id') == agent['id']:
                                received.append(msg)
                            elif msg.get('sender_id') == agent['id']:
                                sent.append(msg)
                    
                    print(f"   üì• Messages re√ßus: {len(received)}")
                    print(f"   üì§ Messages envoy√©s: {len(sent)}")
                    
                    # Afficher les messages envoy√©s et re√ßus
                    if sent:
                        print(f"\n   üì§ Messages envoy√©s:")
                        for msg in sent[:3]:  # Afficher les 3 premiers
                            print(f"      - √Ä: {self._get_agent_name(msg.get('receiver_id'))}")
                            print(f"        Type: {msg.get('performative')}")
                            if isinstance(msg.get('content'), dict):
                                print(f"        Action: {msg['content'].get('action', 'N/A')}")
                    
                    if received:
                        print(f"\n   üì• Messages re√ßus:")
                        for msg in received[:3]:  # Afficher les 3 premiers
                            print(f"      - De: {self._get_agent_name(msg.get('sender_id'))}")
                            print(f"        Type: {msg.get('performative')}")
                            
                    # Chercher les r√©ponses g√©n√©r√©es par LLM
                    for msg in messages:
                        if isinstance(msg, dict):
                            content = msg.get('content', {})
                            if isinstance(content, dict):
                                # Indicateurs de r√©ponse LLM
                                llm_indicators = [
                                    'response', 'answer', 'analysis', 'recommendation',
                                    'proposal', 'evaluation', 'suggestion', 'explanation'
                                ]
                                
                                for indicator in llm_indicators:
                                    if indicator in content:
                                        responses_found += 1
                                        print(f"\n   ü§ñ R√âPONSE LLM D√âTECT√âE!")
                                        print(f"      Type: {msg.get('performative')}")
                                        print(f"      De: {self._get_agent_name(msg.get('sender_id'))}")
                                        print(f"      Vers: {self._get_agent_name(msg.get('receiver_id'))}")
                                        
                                        response_text = str(content[indicator])
                                        print(f"      {indicator.capitalize()}:")
                                        # Afficher les premi√®res lignes de la r√©ponse
                                        lines = response_text.split('\n')
                                        for i, line in enumerate(lines[:5]):
                                            if line.strip():
                                                print(f"         {line.strip()}")
                                        if len(lines) > 5:
                                            print(f"         ... ({len(lines)-5} lignes suppl√©mentaires)")
                                        break
                                        
        return responses_found
        
    def _get_agent_name(self, agent_id: str) -> str:
        """Obtenir le nom d'un agent par son ID"""
        for role, agent in self.agents.items():
            if agent['id'] == agent_id:
                return f"{role.capitalize()} ({agent['name']})"
        return "Unknown"
        
    async def generate_summary(self, responses_found: int, total_cycles: int = 1):
        """G√©n√©rer un r√©sum√© de l'ex√©cution"""
        print("\n" + "="*80)
        print("üìä R√âSUM√â DE L'EX√âCUTION")
        print("="*80)
        
        print(f"\n‚úÖ Utilisateurs cr√©√©s: {len(self.users)}")
        print(f"‚úÖ Agents cr√©√©s: {len(self.agents)}")
        print(f"‚úÖ Cycles ex√©cut√©s: {total_cycles}")
        print(f"‚úÖ Messages envoy√©s total: {len(self.messages_sent)} ({len(self.messages_sent)//total_cycles if total_cycles > 0 else 0} par cycle)")
        print(f"‚úÖ R√©ponses LLM d√©tect√©es: {responses_found}")
        
        print(f"\nüß† Configuration LLM:")
        print(f"   Provider: {os.getenv('LLM_PROVIDER', 'unknown')}")
        print(f"   Model: {os.getenv('LLM_MODEL', 'unknown')}")
        print(f"   Base URL: {os.getenv('LLM_BASE_URL', 'unknown')}")
        
        if responses_found > 0:
            print("\nüéâ SUCC√àS! Les agents ont communiqu√© via LLM!")
            print("   ‚úÖ Les messages ont √©t√© trait√©s par les agents cognitifs")
            print("   ‚úÖ LMStudio a g√©n√©r√© des r√©ponses intelligentes")
            print("   ‚úÖ La communication inter-agents fonctionne parfaitement")
        else:
            print("\n‚ö†Ô∏è  Aucune r√©ponse LLM d√©tect√©e")
            print("   V√©rifiez que:")
            print("   - LMStudio est d√©marr√© avec le mod√®le charg√©")
            print("   - Les agents ont eu assez de temps pour traiter")
            print("   - Le service de livraison des messages fonctionne")
            
    async def cleanup(self):
        """Nettoyage"""
        if self.session:
            await self.session.close()
            
    async def run(self):
        """Ex√©cuter le test complet en boucle continue"""
        try:
            await self.setup()
            await self.create_users()
            await self.create_intelligent_agents()
            await self.start_all_agents()
            
            print("\nüîÑ D√âMARRAGE DU MODE BOUCLE CONTINUE")
            print("   Appuyez sur Ctrl+C pour arr√™ter\n")
            
            cycle_count = 0
            
            while True:
                try:
                    cycle_count += 1
                    print(f"\n{'='*80}")
                    print(f"üîÅ CYCLE #{cycle_count}")
                    print(f"{'='*80}")
                    
                    # Ex√©cuter les sc√©narios
                    await self.complex_scenario_1()
                    await asyncio.sleep(5)
                    
                    await self.complex_scenario_2()
                    await asyncio.sleep(5)
                    
                    await self.complex_scenario_3()
                    
                    # V√©rifier les r√©sultats
                    responses_found = await self.wait_and_check_responses()
                    
                    # Afficher un r√©sum√© rapide du cycle
                    print(f"\nüìä R√©sum√© du cycle #{cycle_count}:")
                    print(f"   - Messages envoy√©s: 3")
                    print(f"   - R√©ponses LLM d√©tect√©es: {responses_found}")
                    print(f"   - Agents actifs: {len(self.agents)}")
                    
                    # Pause entre les cycles
                    print(f"\n‚è∏Ô∏è  Pause de 10 secondes avant le prochain cycle...")
                    await asyncio.sleep(10)
                    
                except KeyboardInterrupt:
                    print("\n\n‚ö†Ô∏è  Interruption d√©tect√©e - Arr√™t du test...")
                    break
                except Exception as e:
                    print(f"\n‚ùå Erreur dans le cycle #{cycle_count}: {str(e)}")
                    print("   Tentative de continuer dans 5 secondes...")
                    await asyncio.sleep(5)
            
            # G√©n√©rer le r√©sum√© final avec le nombre total de cycles
            await self.generate_summary(responses_found, cycle_count)
            
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Interruption d√©tect√©e - Nettoyage en cours...")
        finally:
            await self.cleanup()


async def main():
    test = MASCompleteLLMTest()
    await test.run()


if __name__ == "__main__":
    asyncio.run(main())